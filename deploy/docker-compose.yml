services:
  mcp-git-repo-snapshot:
    build:
      context: ../servers/git-repo-snapshot
      dockerfile: Dockerfile
      args:
        EXTRAS: "sse"
    image: astra/mcp-git-repo-snapshot:local
    container_name: mcp-git-repo-snapshot
    restart: unless-stopped
    environment:
      MCP_TRANSPORT: streamable-http
      MCP_PORT: ${GIT_SNAPSHOT_PORT:-8000}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
    ports:
      - "${GIT_SNAPSHOT_PORT:-8000}:8000"
    volumes:
      - ${WORKSPACE_HOST_DIR:-../data/workspace}:/workspace

  mcp-cobol-parser:
    build:
      context: ../servers/mcp-cobol-parser
      dockerfile: Dockerfile
    image: astra/mcp-cobol-parser:local
    container_name: mcp-cobol-parser
    restart: unless-stopped
    environment:
      # MCP runner config
      MCP_TRANSPORT: streamable-http
      MCP_PORT: ${COBOL_PARSER_PORT:-8765}
      MCP_MOUNT_PATH: /mcp

      # service config
      PAGE_SIZE: 100
      MAX_PAGE_SIZE: 500
      WORKERS: 8
      CACHE_DIR: /app/.cache
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
    ports:
      - "${COBOL_PARSER_PORT:-8765}:8765"
    volumes:
      - ${WORKSPACE_HOST_DIR:-../data/workspace}:/workspace

  mcp-mermaid-diagrammer:
    build:
      context: ../servers/mcp-mermaid-diagrammer
      dockerfile: Dockerfile
    image: astra/mcp-mermaid-diagrammer:local
    container_name: mcp-mermaid-diagrammer
    restart: unless-stopped
    environment:
      MCP_TRANSPORT: streamable-http
      MCP_PORT: ${MERMAID_DIAGRAMMER_PORT:-8001}
      MCP_MOUNT_PATH: /mcp
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      ENABLE_REAL_LLM: "false"
      LLM_PROVIDER: OpenAI
      LLM_MODEL: gpt-4o-mini
      OPENAI_API_KEY: <OPENAI_API_KEY>
    ports:
      - "${MERMAID_DIAGRAMMER_PORT:-8001}:8001"
  
  mcp-workspace-doc-generator:
    build:
      context: ../servers/workspace-doc-generator
      dockerfile: Dockerfile
    image: astra/mcp-workspace-doc-generator:local
    container_name: mcp-workspace-doc-generator
    restart: unless-stopped
    environment:
      # --- MCP Transport ---
      MCP_TRANSPORT: streamable-http
      MCP_PORT: ${WORKSPACE_DOC_PORT:-8002}
      MCP_MOUNT_PATH: /mcp

      # --- Logging ---
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      SERVICE_NAME: mcp.workspace.doc.generator

      # --- LLM (OpenAI) ---
      ENABLE_REAL_LLM: "true"     # set "false" for offline mode
      LLM_PROVIDER: OpenAI
      LLM_MODEL: gpt-4o-mini
      LLM_TEMPERATURE: "0.2"
      LLM_MAX_TOKENS: "10000"
      OPENAI_API_KEY: <OPENAI_API_KEY>

      # --- Services ---
      # Artifact service is in a separate compose; talk to the host-published port
      ARTIFACT_SERVICE_URL: http://host.docker.internal:9020

      # --- Output directory inside the container ---
      OUTPUT_DIR: /out
    ports:
      - "${WORKSPACE_DOC_PORT:-8002}:8002"
    volumes:
      # Persist generated workspace markdown summaries
      - ./data/output/workspace-docs:/out
      # Optional local-fallback context (if you ever drop artifacts.json for a workspace)
      - ${WORKSPACE_HOST_DIR:-../data/workspace}:/workspace
    extra_hosts:
      # <-- this line makes 'host.docker.internal' resolvable in this container
      - "host.docker.internal:host-gateway"